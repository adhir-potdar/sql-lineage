# Lineage Chain Combiner

The Lineage Chain Combiner is a powerful component that combines individual lineage JSON files into consolidated chains, enabling comprehensive analysis of multi-query workflows and data pipeline dependencies.

## Overview

When analyzing complex data pipelines, queries often depend on tables produced by previous queries. The Lineage Chain Combiner identifies these producer-consumer relationships and combines individual lineage files into flowing chains that represent complete data transformation workflows.

### Key Features

✅ **Producer-Consumer Analysis**: Automatically identifies which queries produce tables consumed by other queries  
✅ **Chain Building**: Constructs flowing dependency chains from individual query lineages  
✅ **Connection Point Merging**: Intelligently merges metadata at table connection points  
✅ **Comprehensive Logging**: Built-in logging with configurable levels for all operations  
✅ **Visualization Support**: Generates combined lineage visualizations as JPEG files  
✅ **Error Handling**: Robust error handling with detailed logging for troubleshooting  

## Architecture

### Core Components

- **LineageChainCombiner**: Main class containing all combination logic
- **Connection Point Analysis**: Identifies tables that connect queries in a chain
- **Metadata Merging**: Preserves and combines metadata from producer and consumer queries
- **Chain Visualization**: Generates visual representations of combined chains

### Processing Pipeline

The combiner follows a 3-step process:

1. **Step 1**: Extract complete table chains from each individual lineage JSON
2. **Step 2**: Identify joinable tables and build query chains using producer-consumer relationships
3. **Step 3**: Create combined lineage JSON files with proper connection point merging

## Usage

### Basic Usage

```python
from src.analyzer.lineage_chain_combiner import LineageChainCombiner

# Load your individual lineage JSON files
lineage_data_list = [
    # List of loaded JSON dictionaries
]

# Process through complete pipeline
combined_lineages = LineageChainCombiner.process_lineage_data_complete(lineage_data_list)

# combined_lineages contains combined JSON objects for each query chain
```

### Individual Step Usage

```python
# Step 1: Analyze individual table chains
table_chains, chains_data = LineageChainCombiner.analyze_all_table_chains(lineage_data_list)

# Step 2: Identify joinable connections and build query chains
joinable_connections = LineageChainCombiner.identify_joinable_tables(table_chains)
all_queries = list(table_chains.keys())
query_chains = LineageChainCombiner.identify_joinable_query_chains(joinable_connections, all_queries)

# Step 3: Create combined lineage JSON
combined_lineages = LineageChainCombiner.create_combined_lineage_json(
    query_chains, chains_data, table_chains, joinable_connections
)
```

### Complete Workflow Example

```python
# test_multiple_queries_lineage.py demonstrates the complete workflow
python test_multiple_queries_lineage.py
```

This will:
- Load lineage JSON files from `output/` directory
- Combine them into chains
- Save combined JSON files to `output/combined_output/`
- Generate JPEG visualizations for each combined chain

## Configuration

### Logging Configuration

The combiner uses the centralized logging system. Configure via environment variables or direct setup:

```bash
# Set log level (DEBUG shows detailed step-by-step analysis)
export SQL_LINEAGE_LOG_LEVEL=INFO

# Logging outputs to stdout by default for streaming to external tools
```

### Input Requirements

- **Individual Lineage JSON files**: Generated by `SQLLineageAnalyzer`
- **Proper naming**: Files should follow a consistent naming pattern
- **Complete chains**: Each JSON should contain complete table dependency chains

### Output Structure

Combined lineage JSON files contain:
- **Merged SQL**: Combined SQL statements from all queries in the chain
- **Flowing dependencies**: Single chain showing complete data flow
- **Connection point metadata**: Merged metadata at table connection points
- **Summary statistics**: Aggregated information about the combined chain

## Connection Point Merging

### What are Connection Points?

Connection points are tables that serve as the output of one query and input of another query in the same chain.

### Merging Process

1. **Identification**: Find tables that connect consecutive queries
2. **Metadata Merging**: Combine metadata from both producer and consumer perspectives
3. **Dependency Extension**: Extend the connection point with consumer query processing
4. **Preservation**: Maintain all transformation and metadata information

### Example

```
Query 1: orders → recent_orders → filtered_orders (produces filtered_orders table)
Query 2: filtered_orders → customer_stats → customer_summary (consumes filtered_orders table)

Connection Point: filtered_orders
- Contains metadata from Query 1 (producer perspective)  
- Extended with Query 2 processing (consumer perspective)
- Preserves all transformations and column information
```

## Visualization

The combiner integrates with the visualization system to generate JPEG diagrams:

```python
from src.analyzer.visualization.visualizer import SQLLineageVisualizer

visualizer = SQLLineageVisualizer()

# Create visualization for combined lineage
jpeg_file = visualizer.create_lineage_chain_diagram(
    combined_lineage_json,
    output_path="combined_lineage",
    output_format="jpeg"
)
```

### Visualization Features

- **Flow Direction**: Shows data flow from source to target tables
- **Connection Points**: Highlights tables that connect queries
- **CTE Representation**: Displays Common Table Expressions within queries
- **Metadata Integration**: Shows column information and transformations

## Error Handling

### Common Issues

**Empty Dependencies**: When connection points have empty dependencies
- **Cause**: Bug in original lineage analysis or incomplete data
- **Handling**: Logged as warnings, processing continues

**Multiple Producers**: When a table is produced by multiple queries
- **Cause**: Conflicting table creation in different queries
- **Handling**: Warning logged, last producer wins

**Missing Consumer Data**: When referenced consumer query data is not found
- **Cause**: Incomplete input data or naming mismatches
- **Handling**: Error logged, connection point merging skipped

### Debugging

Enable DEBUG logging to see detailed step-by-step analysis:

```bash
export SQL_LINEAGE_LOG_LEVEL=DEBUG
python test_multiple_queries_lineage.py
```

This provides detailed logs of:
- Table chain extraction from each JSON
- Producer-consumer relationship identification
- Connection point analysis and merging
- Chain building progress

## API Reference

### LineageChainCombiner

```python
class LineageChainCombiner:
    @classmethod
    def process_lineage_data_complete(cls, lineage_data_list: List[Dict]) -> List[Dict]
        """Complete 3-step processing pipeline"""
    
    @classmethod  
    def analyze_all_table_chains(cls, lineage_data_list: List[Dict]) -> Tuple[Dict[str, List[str]], Dict[str, Dict]]
        """Step 1: Extract table chains from individual JSON files"""
    
    @classmethod
    def identify_joinable_tables(cls, table_chains: Dict[str, List[str]]) -> Dict[str, Dict[str, List[str]]]
        """Step 2 Part 1: Identify producer-consumer relationships"""
    
    @classmethod
    def identify_joinable_query_chains(cls, joinable_connections: Dict, all_queries: List[str]) -> List[List[str]]
        """Step 2 Part 2: Build query chains from connections"""
    
    @classmethod
    def create_combined_lineage_json(cls, query_chains: List[List[str]], chains_data: Dict, 
                                   table_chains: Dict, joinable_connections: Dict) -> List[Dict]
        """Step 3: Create combined lineage JSON with connection point merging"""
```

## Testing

Test the lineage combiner functionality:

```bash
# Run the complete multi-query analysis workflow
python test_multiple_queries_lineage.py

# Check output files
ls -la output/combined_output/

# View combined JSON structure  
cat output/combined_output/combined_chain_1.json | jq .

# Check generated visualizations
ls -la output/combined_output/*.jpeg
```

## Best Practices

### Input Preparation

1. **Consistent Naming**: Use consistent table naming across queries
2. **Complete Analysis**: Ensure individual lineage JSONs contain complete chains
3. **Proper Sequencing**: Organize queries in logical execution order

### Performance Considerations

1. **Batch Processing**: Process multiple files together for efficiency
2. **Memory Usage**: Large datasets may require streaming processing
3. **Logging Levels**: Use INFO level in production, DEBUG for troubleshooting

### Output Management

1. **Directory Structure**: Organize combined outputs in separate directories
2. **File Naming**: Use descriptive names for combined chain files
3. **Visualization Storage**: Store JPEG files alongside JSON for reference

## Troubleshooting

### Common Problems

**No Combined Chains Generated**
- Check that input JSONs contain valid table chains
- Verify table naming consistency between queries
- Enable DEBUG logging to trace the analysis

**Missing Connection Points**
- Ensure producer-consumer table names match exactly
- Check for schema/catalog prefixes in table names
- Verify query execution order in your pipeline

**Visualization Errors**
- Ensure Graphviz system package is installed
- Check that combined JSON structure is valid
- Verify output directory permissions

### Debug Commands

```bash
# Enable detailed logging
export SQL_LINEAGE_LOG_LEVEL=DEBUG

# Check input file structure
python -c "
import json
with open('output/file.json') as f:
    data = json.load(f)
    print('Chains:', list(data.get('chains', {}).keys()))
"

# Validate combined output
python -c "
import json
with open('output/combined_output/combined_chain_1.json') as f:
    data = json.load(f)
    print('Chain type:', data.get('chain_type'))
    print('Entities:', len(data.get('chains', {})))
"
```

## Integration

### With Data Catalogs

The combined lineage JSON can be integrated with data catalog systems:

```python
# Export to your data catalog
for combined_lineage in combined_lineages:
    catalog_client.register_lineage(
        lineage_data=combined_lineage,
        pipeline_id=f"combined_chain_{i}",
        metadata=combined_lineage['summary']
    )
```

### With Workflow Systems

Use combined chains to understand complete workflow dependencies:

```python
# Generate workflow dependency graph
workflow_graph = build_workflow_graph(combined_lineages)
```

---

**For more information, see the main [README.md](../README.md) for general SQL lineage analysis capabilities.**